import requests
from bs4 import BeautifulSoup
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core import Settings
from openai import OpenAI
import os
import streamlit as st
import json

# OpenAIã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨
from llama_index.embeddings.openai import OpenAIEmbedding

Settings.embed_model = OpenAIEmbedding(model="text-embedding-ada-002")


# ä¿å­˜ç”¨ã®JSONãƒ•ã‚¡ã‚¤ãƒ«
data_file = "saved_links.json"

# Webãƒšãƒ¼ã‚¸ã‚’å–å¾—ã™ã‚‹é–¢æ•°
def fetch_web_content(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        text = ' '.join([p.get_text() for p in soup.find_all("p")])
        return text
    else:
        return None

# è¨˜äº‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜ã—ã€RAGã«ç™»éŒ²
def index_web_contents(urls, save_dir="web_articles"):
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    for idx, url in enumerate(urls):
        content = fetch_web_content(url)
        if content:
            filename = os.path.join(save_dir, f"article_{idx}.txt")
            with open(filename, "w", encoding="utf-8") as f:
                f.write(content)
    
    # LlamaIndexã«ç™»éŒ²
    documents = SimpleDirectoryReader(save_dir).load_data()
    index = VectorStoreIndex.from_documents(documents)
    index.storage_context.persist()
    
    return index

# ChatGPTã¨ã®é€£æº
def ask_chatbot_with_web(question, index):
    from llama_index.core.query_engine import RetrieverQueryEngine
    query_engine = RetrieverQueryEngine(index)
    context = query_engine.query(question)
    
    response = OpenAI.ChatCompletion.create(
        model="gpt-4o mini",
        messages=[
            {"role": "system", "content": f"Use this context: {context}"},
            {"role": "user", "content": question}
        ]
    )
    
    return response["choices"][0]["message"]["content"]

# ä¿å­˜æ¸ˆã¿ã®ãƒªãƒ³ã‚¯ã‚’èª­ã¿è¾¼ã‚€
def load_saved_links():
    if os.path.exists(data_file):
        with open(data_file, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

# ãƒªãƒ³ã‚¯ã‚’ä¿å­˜ã™ã‚‹
def save_links(category, links):
    data = load_saved_links()
    if category not in data:
        data[category] = []
    data[category].extend(links)
    with open(data_file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4)

# Streamlit UI
st.title("Webãƒªãƒ³ã‚¯ã‚’RAGã«ç™»éŒ²ã—ã€AIã«è³ªå•")

categories = st.text_input("ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’å…¥åŠ›")
urls = st.text_area("Webãƒªãƒ³ã‚¯ã‚’å…¥åŠ›ï¼ˆæ”¹è¡Œã§è¤‡æ•°è¿½åŠ ï¼‰")
question = st.text_input("AIã«è³ªå•ã™ã‚‹å†…å®¹")

if st.button("ç™»éŒ²ï¼†æ¤œç´¢"):
    url_list = [url.strip() for url in urls.split("\n") if url.strip()]
    if url_list and categories:
        save_links(categories, url_list)
        index = index_web_contents(url_list)
        if question:
            answer = ask_chatbot_with_web(question, index)
            st.write("### å›ç­”:")
            st.write(answer)
        else:
            st.write("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        st.write("æœ‰åŠ¹ãªURLã¨ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

# ã‚®ãƒ£ãƒ©ãƒªãƒ¼è¡¨ç¤º
data = load_saved_links()
st.write("### ä¿å­˜æ¸ˆã¿ã®ãƒªãƒ³ã‚¯")
for category, links in data.items():
    with st.expander(category):
        for link in links:
            st.markdown(f"[ğŸ”— {link}]({link})")
